{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a08b7ee4",
   "metadata": {},
   "source": [
    "## Creating a K8S Cluster on an M1 Mac\n",
    "\n",
    "At this point in time, Macs with the M1 line of processors work best with *k3d*, which installs k3s inside of containers runnign in Docker.  \n",
    "\n",
    "We start by ensuring we have not already created our cluster:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e4f7f",
   "metadata": {},
   "source": [
    "Install Docker:\n",
    "\n",
    "https://docs.docker.com/desktop/mac/install/\n",
    "(Select Mac with Apple Chip)\n",
    "\n",
    "Follow the installation instructions to install Docker Desktop for your OS.\n",
    "\n",
    "After Docker destkop has been installed, launch docker desktop then proceed with installing k3d:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57abff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/k3d/manifests/5.2.2\u001b[0m\n",
      "######################################################################## 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/k3d/blobs/sha256:94ab53847ee937\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh\u001b[0m\n",
      "######################################################################## 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring k3d--5.2.2.arm64_big_sur.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "zsh completions have been installed to:\n",
      "  /opt/homebrew/share/zsh/site-functions\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/k3d/5.2.2: 9 files, 18.9MB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup k3d`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
     ]
    }
   ],
   "source": [
    "brew install k3d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9e507c",
   "metadata": {},
   "source": [
    "Ensure k3d was installed correctly with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "669b95cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k3d\n"
     ]
    }
   ],
   "source": [
    "brew list | grep -i k3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53eb6a1b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME   SERVERS   AGENTS   LOADBALANCER\n"
     ]
    }
   ],
   "source": [
    "k3d cluster list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fff7fc",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "The following command creates a cluster named *test*.  It then maps localhost ports 8081 and 8443 to the cluster's LoadBalancer service on ports 80 and 443, where Traefik is running as an ingress controller.  It creates a cluster with 1 control-plane node and 2 worker/agent nodes, exposing the control plane using an API server running on port 6550:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c2aad55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36mINFO\u001b[0m[0000] portmapping '8443:443' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy] \n",
      "\u001b[36mINFO\u001b[0m[0000] portmapping '8081:80' targets the loadbalancer: defaulting to [servers:*:proxy agents:*:proxy] \n",
      "\u001b[36mINFO\u001b[0m[0000] Prep: Network                                \n",
      "\u001b[36mINFO\u001b[0m[0000] Created network 'k3d-test'                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Created volume 'k3d-test-images'             \n",
      "\u001b[36mINFO\u001b[0m[0000] Starting new tools node...                   \n",
      "\u001b[36mINFO\u001b[0m[0000] Starting Node 'k3d-test-tools'               \n",
      "\u001b[36mINFO\u001b[0m[0001] Creating node 'k3d-test-server-0'            \n",
      "\u001b[36mINFO\u001b[0m[0001] Creating node 'k3d-test-agent-0'             \n",
      "\u001b[36mINFO\u001b[0m[0001] Creating node 'k3d-test-agent-1'             \n",
      "\u001b[36mINFO\u001b[0m[0001] Creating LoadBalancer 'k3d-test-serverlb'    \n",
      "\u001b[36mINFO\u001b[0m[0001] Using the k3d-tools node to gather environment information \n",
      "\u001b[36mINFO\u001b[0m[0002] Starting cluster 'test'                      \n",
      "\u001b[36mINFO\u001b[0m[0002] Starting servers...                          \n",
      "\u001b[36mINFO\u001b[0m[0002] Starting Node 'k3d-test-server-0'            \n",
      "\u001b[36mINFO\u001b[0m[0006] Starting agents...                           \n",
      "\u001b[36mINFO\u001b[0m[0006] Starting Node 'k3d-test-agent-1'             \n",
      "\u001b[36mINFO\u001b[0m[0007] Starting Node 'k3d-test-agent-0'             \n",
      "\u001b[36mINFO\u001b[0m[0019] Starting helpers...                          \n",
      "\u001b[36mINFO\u001b[0m[0019] Starting Node 'k3d-test-serverlb'            \n",
      "\u001b[36mINFO\u001b[0m[0026] Injecting '192.168.65.2 host.k3d.internal' into /etc/hosts of all nodes... \n",
      "\u001b[36mINFO\u001b[0m[0026] Injecting records for host.k3d.internal and for 4 network members into CoreDNS configmap... \n",
      "\u001b[36mINFO\u001b[0m[0027] Cluster 'test' created successfully!         \n",
      "\u001b[36mINFO\u001b[0m[0027] You can now use it like this:                \n",
      "kubectl cluster-info\n"
     ]
    }
   ],
   "source": [
    "k3d cluster create --api-port 6550 -p \"8081:80@loadbalancer\" -p \"8443:443@loadbalancer\" --agents 2 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d0369a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                STATUS   ROLES                  AGE   VERSION\n",
      "k3d-test-agent-1    Ready    <none>                 22s   v1.21.7+k3s1\n",
      "k3d-test-agent-0    Ready    <none>                 22s   v1.21.7+k3s1\n",
      "k3d-test-server-0   Ready    control-plane,master   31s   v1.21.7+k3s1\n"
     ]
    }
   ],
   "source": [
    "kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d32dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                                      READY   STATUS              RESTARTS   AGE\n",
      "kube-system   local-path-provisioner-5ff76fc89d-rdhhg   1/1     Running             0          30s\n",
      "kube-system   metrics-server-86cbb8457f-f9sbq           1/1     Running             0          30s\n",
      "kube-system   coredns-7448499f4d-9fwq4                  1/1     Running             0          30s\n",
      "kube-system   helm-install-traefik-crd-98st8            0/1     Completed           0          31s\n",
      "kube-system   traefik-6b84f7cbc-mznc4                   0/1     ContainerCreating   0          3s\n",
      "kube-system   svclb-traefik-mm4qb                       0/2     ContainerCreating   0          3s\n",
      "kube-system   svclb-traefik-v84tv                       0/2     ContainerCreating   0          3s\n",
      "kube-system   svclb-traefik-kxl6r                       0/2     ContainerCreating   0          3s\n",
      "kube-system   helm-install-traefik-cs2bw                0/1     Completed           2          31s\n"
     ]
    }
   ],
   "source": [
    "kubectl get pods -A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72469687",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## dnsmasq\n",
    "\n",
    "From this point, we'd like to have DNS resolve certain DNS names to localhost for our cluster.  For this, we're going to use *dnsmasq*, which allows us to override wildcard DNS entries and point them to localhost.  Since we're on a Mac, we're going to use 'brew' to install it.  In this case, we've already installed it, so no changes are made:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12917558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/dnsmasq/manifests/2.86\u001b[0m\n",
      "######################################################################## 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/core/dnsmasq/blobs/sha256:958b73b470\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sh\u001b[0m\n",
      "######################################################################## 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring dnsmasq--2.86.arm64_big_sur.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "To restart dnsmasq after an upgrade:\n",
      "  sudo brew services restart dnsmasq\n",
      "Or, if you don't want/need a background service you can just run:\n",
      "  /opt/homebrew/opt/dnsmasq/sbin/dnsmasq --keep-in-foreground -C /opt/homebrew/etc/dnsmasq.conf -7 /opt/homebrew/etc/dnsmasq.d,*.conf\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "🍺  /opt/homebrew/Cellar/dnsmasq/2.86: 10 files, 614.9KB\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup dnsmasq`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n"
     ]
    }
   ],
   "source": [
    "brew install dnsmasq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a518e6c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dnsmasq\n"
     ]
    }
   ],
   "source": [
    "brew list | grep -i dnsmasq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d4e8e9",
   "metadata": {},
   "source": [
    "In your Jupyter notebook working directory, create a file called \"password\" and put your root password in this file. This is to enable Jupyter notebook to execute sudo commands without exposing your root password or storing it in your machines CLI history while creating a variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bebf7c0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next we will add a rule to the configuration for dnsmasq that routes any URL ending in *.test* to 127.0.0.1.  The code below checks first to see if that line already exists and adds it if it does not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d291162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DNSMasq rule for .test top-level domain already exists\n"
     ]
    }
   ],
   "source": [
    "if grep -Fxq \"address=/.test/127.0.0.1\" /opt/homebrew/etc/dnsmasq.conf\n",
    "then\n",
    "    echo \"DNSMasq rule for .test top-level domain already exists\"\n",
    "else\n",
    "    cat password | sudo -S echo \"address=/.test/127.0.0.1\" >> /opt/homebrew/etc/dnsmasq.conf\n",
    "    grep \"address=/.test/127.0.0.1\" /opt/homebrew/etc/dnsmasq.conf\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eef9bd53",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "From here we will need to use *brew* to restart the dnsmasq service so that the new configuration can be loaded & used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f71ab486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mTapping homebrew/services\u001b[0m\n",
      "Cloning into '/opt/homebrew/Library/Taps/homebrew/homebrew-services'...\n",
      "remote: Enumerating objects: 1656, done.\u001b[K\n",
      "remote: Counting objects: 100% (535/535), done.\u001b[K\n",
      "remote: Compressing objects: 100% (392/392), done.\u001b[K\n",
      "remote: Total 1656 (delta 229), reused 356 (delta 130), pack-reused 1121\u001b[K\n",
      "Receiving objects: 100% (1656/1656), 481.02 KiB | 1.57 MiB/s, done.\n",
      "Resolving deltas: 100% (705/705), done.\n",
      "Tapped 1 command (44 files, 616.0KB).\n",
      "\u001b[33mWarning:\u001b[0m Taking root:admin ownership of some dnsmasq paths:\n",
      "  /opt/homebrew/Cellar/dnsmasq/2.86/sbin\n",
      "  /opt/homebrew/Cellar/dnsmasq/2.86/sbin/dnsmasq\n",
      "  /opt/homebrew/opt/dnsmasq\n",
      "  /opt/homebrew/opt/dnsmasq/sbin\n",
      "  /opt/homebrew/var/homebrew/linked/dnsmasq\n",
      "This will require manual removal of these paths using `sudo rm` on\n",
      "brew upgrade/reinstall/uninstall.\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSuccessfully started `dnsmasq` (label: homebrew.mxcl.dnsmasq)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "cat password | sudo -S brew services restart dnsmasq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c53cd0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Use `dig` to test that URLs ending in `.test` resolve to 127.0.0.1 on the DNS server running at 127.0.0.1.  The `@127.0.0.1` option tells dig to use the DNS server running on localhost, the `dnsmasq` service we started earlier.  Note that the `ANSWER SECTION` provides the response for `fubar.test` as `127.0.0.1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "da0a8f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "; <<>> DiG 9.10.6 <<>> @127.0.0.1 fubar.test\n",
      "; (1 server found)\n",
      ";; global options: +cmd\n",
      ";; Got answer:\n",
      ";; ->>HEADER<<- opcode: QUERY, status: NOERROR, id: 8691\n",
      ";; flags: qr aa rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1\n",
      "\n",
      ";; OPT PSEUDOSECTION:\n",
      "; EDNS: version: 0, flags:; udp: 4096\n",
      ";; QUESTION SECTION:\n",
      ";fubar.test.\t\t\tIN\tA\n",
      "\n",
      ";; ANSWER SECTION:\n",
      "fubar.test.\t\t0\tIN\tA\t127.0.0.1\n",
      "\n",
      ";; Query time: 0 msec\n",
      ";; SERVER: 127.0.0.1#53(127.0.0.1)\n",
      ";; WHEN: Sat Jan 08 13:14:41 MST 2022\n",
      ";; MSG SIZE  rcvd: 55\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dig @127.0.0.1 fubar.test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1bbe00",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Next, we'll want to override the Mac's DNS resolver for all URLs ending in `.test`, sending only those URLs to our dnsmasq service.  The mac uses \"/etc/resolver\" for this purpose.  This will allow us to use any browser, such as Chrome or Safari, to use `dnsmasq` to resolve URLs ending in `.test`.\n",
    "\n",
    "If there is a file in `/etc/resolver` with a filename that matches the end of the URL, then the contents of that file will identify the DNS server to use for that URL.  In our case, we create a file named `/etc/resolver/test` that configures handling of all URLs ending in `.test`, a fictitious top-level domain that I created for test purposes.  This easily could be a subdomain of an actual domain owned by the cluster operator (for example, to listen to any URLs ending in `.k8s.example.com`, the full filename would be `/etc/resolver/k8s.example.com`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c812ecd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat password | sudo -S mkdir -p /etc/resolver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "69890f86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grep: /etc/resolver/test: No such file or directory\n",
      "Password:nameserver 127.0.0.1\n"
     ]
    }
   ],
   "source": [
    "if grep -Fxq \"nameserver 127.0.0.1\" /etc/resolver/test\n",
    "then\n",
    "    echo \"MacOS DNS resolver for .test top-level domain already defers to localhost\"\n",
    "else\n",
    "    cat password | sudo -S sh -c 'echo \"nameserver 127.0.0.1\" >> /etc/resolver/test'\n",
    "    cat /etc/resolver/test\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e879e247",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "### Testing\n",
    "We will test that the entire process works from end-to-end by deploying an NGINX pod to our local k8s cluster, exposing it using an Ingress resource, and then access the pod in our web browser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75604ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "brew install kubectl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6d56f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps/nginx created\n"
     ]
    }
   ],
   "source": [
    "kubectl create deployment nginx --image=nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4fbc0d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service/nginx created\n"
     ]
    }
   ],
   "source": [
    "kubectl create service clusterip nginx --tcp=80:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5dba2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat << EOF > ingress.yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: nginx\n",
    "  annotations:\n",
    "    ingress.kubernetes.io/ssl-redirect: \"false\"\n",
    "spec:\n",
    "  rules:\n",
    "  - host: nginx.test\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: nginx\n",
    "            port:\n",
    "              number: 80\n",
    "EOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "427401a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingress.networking.k8s.io/nginx created\n"
     ]
    }
   ],
   "source": [
    "kubectl apply -f ingress.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16f2418",
   "metadata": {},
   "source": [
    "<br>\n",
    "Recall that when we started our k3d cluster, we mapped the cluster's ingress controller service's port 80 & 443 to localhost ports 8081 and 8443.  We also configured 'dnsmasq' to resolve all URLs ending in '.test' to 127.0.0.1.  This allows us to use our web browser to visit HTTP & HTTPS services exposed using ingress objects inside our local k3d cluster.\n",
    "\n",
    "Open a browser window to [http://nginx.test:8081](http://nginx.test:8081) and confirm that you can see the NGINX starter page.  Likewise, you will be able to visit https://nginx.test:8443 (after accepting TLS certificate errors) and also see the same page.  Likewise, confirm that http://bad-url.test:8081 also resolves to the ingress controller, but gives a `404 page not found` error.\n",
    "<br>\n",
    "### Cleanup\n",
    "After confirming that the cluster works as expected, let's remove our test instance of nginx:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d4606f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ingress.networking.k8s.io \"nginx\" deleted\n"
     ]
    }
   ],
   "source": [
    "kubectl delete ingress nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12c0983a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service \"nginx\" deleted\n"
     ]
    }
   ],
   "source": [
    "kubectl delete service nginx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d21b3ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.apps \"nginx\" deleted\n"
     ]
    }
   ],
   "source": [
    "kubectl delete deployment nginx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5fee9a",
   "metadata": {},
   "source": [
    "<br>\n",
    "Our local cluster is now ready for use.  This cluster can be used for all of our Nephtek workbooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
